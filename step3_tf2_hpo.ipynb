{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Tensorflow 컨테이너를 활용한 하이퍼 파라미터 튜닝\n",
    "\n",
    "이 학습서는 **SageMaker TensorFlow 컨테이너**를 사용하여 디바이스 장애 데이터 세트를 학습하기 위해 컨볼 루션 신경망 모델을 작성하는 방법에 중점을 둡니다. 하이퍼 파라미터 튜닝을 활용하여 다양한 하이퍼 파라미터 조합으로 여러 교육 작업을 시작하여 최상의 모델 교육 결과를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "워크 플로를 시작하기 전에 몇 가지 사항을 설정하겠습니다.\n",
    "\n",
    "1. 학습 데이터 세트 및 모델 아티팩트가 저장 될 s3 버킷을 지정합니다.\n",
    "2. s3 버킷과 같은 리소스에 액세스하기 위해 sagemaker에 전달 될 실행 역할을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = 'chris.loves.ai' # we are using a default bucket here but you can change it to any bucket in your account\n",
    "\n",
    "role = sagemaker.get_execution_role() # we are using the notebook instance role for training in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 필요한 Python 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 업로드\n",
    "``sagemaker.Session.upload_data`` 함수를 사용하여 데이터 셋을 S3 위치에 업로드합니다. 반환 값은 위치를 식별합니다. 나중에 훈련 작업을 시작할 때이 값을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-318688069356/sagemaker/DEMO-hpo-tensorflow-high/data/mnist\n"
     ]
    }
   ],
   "source": [
    "# inputs = sagemaker.Session().upload_data(path='data', bucket=bucket, key_prefix='/dataset/device-failure')\n",
    "# print (inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3341  100  3341    0     0  11211      0 --:--:-- --:--:-- --:--:-- 11173\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/chris-chris/sagemaker-mlops/master/tf2.py > tf2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분산 훈련을위한 스크립트 작성\n",
    "\n",
    "네트워크 모델의 전체 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"Tensorflow2 Keras Version Model Training\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Dropout\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.models\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Sequential\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.callbacks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m EarlyStopping, LambdaCallback\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.optimizers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Adam\r\n",
      "\r\n",
      "BATCH_SIZE = \u001b[34m64\u001b[39;49;00m\r\n",
      "SHUFFLE_BUFFER_SIZE = \u001b[34m100\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel\u001b[39;49;00m(train_dataset, test_dataset, args):\r\n",
      "  \u001b[33m\"\"\"Generate a simple model\"\"\"\u001b[39;49;00m\r\n",
      "  model = Sequential(\r\n",
      "      [\r\n",
      "        Dense(args.fc1, input_shape=(\u001b[34m8\u001b[39;49;00m,)),\r\n",
      "        Dense(args.fc2, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "              kernel_regularizer=tf.keras.regularizers.l2(\u001b[34m0.01\u001b[39;49;00m)),\r\n",
      "        Dropout(args.dropout),\r\n",
      "        Dense(\u001b[34m1\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "      ])\r\n",
      "  \u001b[37m# eval_callback = LambdaCallback(\u001b[39;49;00m\r\n",
      "  \u001b[37m#     on_epoch_end=lambda epoch, logs: logs.update(\u001b[39;49;00m\r\n",
      "  \u001b[37m#         {'mean_logits': K.eval(mean)}\u001b[39;49;00m\r\n",
      "  \u001b[37m#     ))\u001b[39;49;00m\r\n",
      "  model.compile(optimizer=Adam(learning_rate=args.learning_rate), loss=\u001b[33m'\u001b[39;49;00m\u001b[33mbinary_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "  early_stopping = EarlyStopping(monitor=\u001b[33m'\u001b[39;49;00m\u001b[33mval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, patience=\u001b[34m10\u001b[39;49;00m)\r\n",
      "\r\n",
      "  model.fit(\r\n",
      "      train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE),\r\n",
      "      validation_data=test_dataset.batch(BATCH_SIZE),\r\n",
      "      epochs=\u001b[34m100\u001b[39;49;00m, verbose=\u001b[34m0\u001b[39;49;00m, callbacks=[early_stopping])\r\n",
      "\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_data\u001b[39;49;00m(base_dir):\r\n",
      "  \u001b[33m\"\"\"Load device failure data\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  total_data = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtotal_data.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "  total_label = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtotal_label.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "  test_split = \u001b[36mint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(total_data)*\u001b[34m0.2\u001b[39;49;00m)\r\n",
      "\r\n",
      "  train_data = total_data[:-test_split]   \u001b[37m# 8:2\u001b[39;49;00m\r\n",
      "  train_label = total_label[:-test_split] \u001b[37m# 8:2\u001b[39;49;00m\r\n",
      "  test_data = total_data[-test_split:]   \u001b[37m# 8:2\u001b[39;49;00m\r\n",
      "  test_label = total_label[-test_split:] \u001b[37m# 8:2\u001b[39;49;00m\r\n",
      "\r\n",
      "  train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label))\r\n",
      "  test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_label))\r\n",
      "\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m train_dataset, test_dataset\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\r\n",
      "  parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "  \u001b[37m# Data, model, and output directories\u001b[39;49;00m\r\n",
      "  \u001b[37m# model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\u001b[39;49;00m\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m)\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--fc1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--fc2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\r\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m)\r\n",
      "\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "  args, unknown = _parse_args()\r\n",
      "\r\n",
      "  \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mars: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, args)\r\n",
      "  \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning_rate: {args.learning_rate}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "  train_dataset, test_dataset = _load_data(args.train)\r\n",
      "\r\n",
      "  device_failure_model = model(train_dataset, test_dataset, args)\r\n",
      "\r\n",
      "  loss = device_failure_model.evaluate(test_dataset.batch(BATCH_SIZE))\r\n",
      "  \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_loss: {loss}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "  tf.summary.scalar(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, loss)\r\n",
      "\r\n",
      "  \u001b[34mif\u001b[39;49;00m args.current_host == args.hosts[\u001b[34m0\u001b[39;49;00m]:\r\n",
      "    \u001b[37m# save model to an S3 directory with version number '00000001'\u001b[39;49;00m\r\n",
      "    device_failure_model.save(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33m000000001\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mmy_model.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 2.1 script\n",
    "!pygmentize 'tf2.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 튜닝 작업 설정\n",
    "* 아래의 기본 설정에서 하이퍼 파라미터 튜닝 작업을 완료하는 데 약 30 분이 걸릴 수 있습니다. *\n",
    "\n",
    "이제 다음 단계에 따라 SageMaker Python SDK를 사용하여 하이퍼 파라미터 튜닝 작업을 설정합니다.\n",
    "\n",
    "* TensorFlow 교육 작업을 설정하기위한 추정기를 작성하십시오.\n",
    "* 튜닝하려는 하이퍼 파라미터의 범위를 정의합니다.이 예에서는 \"learning_rate\"를 튜닝합니다.\n",
    "* 튜닝 작업을위한 객관적인 메트릭 정의\n",
    "* 위의 설정과 튜닝 리소스 구성으로 하이퍼 파라미터 튜너를 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker에서 단일 TensorFlow 작업을 학습하는 것과 유사하게 TensorFlow 스크립트, IAM 역할 및 (작업 별) 하드웨어 구성을 전달하는 TensorFlow 추정기를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_failure_estimator = TensorFlow(entry_point='tf2.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='ml.p2.xlarge',\n",
    "                             framework_version='2.1.0',\n",
    "                             py_version='py3',\n",
    "                             distributions={'parameter_server': {'enabled': True}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator를 정의한 후에는 조정하려는 하이퍼 파라미터와 가능한 값을 지정할 수 있습니다. 하이퍼 파라미터에는 세 가지 유형이 있습니다.\n",
    "-범주 형 매개 변수는 이산 세트에서 하나의 값을 가져와야합니다. 가능한 값 목록을`CategoricalParameter (list)`에 전달하여이를 정의합니다.\n",
    "-연속 매개 변수는`ContinuousParameter (min, max)`에 의해 정의 된 최소값과 최대 값 사이의 임의의 실수 값을 취할 수 있습니다.\n",
    "-정수 매개 변수는`IntegerParameter (min, max)`에 의해 정의 된 최소값과 최대 값 사이의 정수 값을 가질 수 있습니다\n",
    "\n",
    "* 가능하면 값을 최소 제한 유형으로 지정하는 것이 가장 좋습니다. 예를 들어 학습 속도를 0.01과 0.2 사이의 연속 값으로 조정하면 값이 0.01, 0.1, 0.15 또는 0.2 인 범주 형 매개 변수로 조정하는 것보다 더 나은 결과를 얻을 수 있습니다. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.01, 0.2),\n",
    "    'dropout': ContinuousParameter(0.01, 0.5),\n",
    "    'fc1': CategoricalParameter([32,64,128,256,512]),\n",
    "    'fc2': CategoricalParameter([32,64,128,256]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 조정하려는 객관적인 지표와 그 정의를 지정합니다. 여기에는 훈련 작업의 CloudWatch 로그에서 해당 지표를 추출하는 데 필요한 정규식 (Regex)이 포함됩니다. 이 특별한 경우 스크립트에서 손실 값을 생성하고이를 객관적인 메트릭으로 사용하고 objective_type을 '최소화'로 설정하여 하이퍼 파라미터 튜닝이 최상의 하이퍼 파라미터 설정을 검색 할 때 객관적인 메트릭을 최소화하도록합니다. 기본적으로 objective_type은 'maximize'로 설정되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'test_loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'test_loss',\n",
    "                       'Regex': 'test_loss: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 전달할`HyperparameterTuner` 객체를 만들 것입니다 :\n",
    "\n",
    "- 위에서 만든 TensorFlow Estimator\n",
    "- 하이퍼 파라미터 범위\n",
    "- 객관적인 메트릭 이름 및 정의\n",
    "- 실행할 총 교육 작업 수 및 병렬로 실행할 수있는 교육 작업 수와 같은 튜닝 리소스 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(device_failure_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=3,\n",
    "                            max_parallel_jobs=1,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 튜닝 작업 시작\n",
    "\n",
    "마지막으로`.fit ()`을 호출하고 S3 경로를 기차 및 테스트 데이터 세트로 전달하여 하이퍼 프라 미터 튜닝 작업을 시작할 수 있습니다.\n",
    "\n",
    "하이퍼 파라미터 튜닝 작업이 생성 된 후 다음 단계에서 진행 상황을 확인하기 위해 튜닝 작업을 설명 할 수 있어야하며 SageMaker 콘솔-> 작업으로 이동하여 하이퍼 파라미터 튜닝 작업의 진행 상황을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_uri = 's3://chris.loves.ai/dataset/device-failure'\n",
    "tuner.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 튜닝 작업 상태를 빠르게 확인하여 성공적으로 시작되었는지 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'InProgress'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 튜닝 작업 결과 분석-튜닝 작업이 완료된 후\n",
    "\n",
    "튜닝 작업 결과를 분석하는 예제 코드를 보려면 SageMaker 웹 콘솔을 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최고의 모델 배포\n",
    "\n",
    "최고의 모델을 얻었으므로 이제 엔드 포인트에 배포 할 수 있습니다. 모델을 배포하는 방법을 보려면 다른 SageMaker 샘플 노트북 또는 SageMaker 설명서를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
